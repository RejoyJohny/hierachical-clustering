{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ef9c2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\rejoy\\miniconda3\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rejoy\\miniconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rejoy\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/11.5 MB 8.5 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.7/11.5 MB 12.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.8/11.5 MB 13.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.7/11.5 MB 13.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 13.1 MB/s eta 0:00:00\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   ---------------------------------------- 3/3 [pandas]\n",
      "\n",
      "Successfully installed pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3d73d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1: https://www.karkidi.com/Find-Jobs/1/all/India?search=data%20science\n",
      "Scraping page 2: https://www.karkidi.com/Find-Jobs/2/all/India?search=data%20science\n",
      "Saved to karkidi_jobs.csv\n",
      "                                               Title      Company  \\\n",
      "0   Principal Product Manager - Growth, Poe (Remote)  Quora, Inc.   \n",
      "1          Machine Learning Physical Design Engineer       Google   \n",
      "2  Staff Software Engineer - Monetization, Poe (R...  Quora, Inc.   \n",
      "3  Staff Backend Engineer - Bot Creator Ecosystem...  Quora, Inc.   \n",
      "4  Senior Backend Engineer - Bot Creator Ecosyste...  Quora, Inc.   \n",
      "\n",
      "                      Location Experience  \\\n",
      "0                        India   6-8 year   \n",
      "1  Bengaluru, Karnataka, India   4-6 year   \n",
      "2                        India  8-10 year   \n",
      "3                        India  8-10 year   \n",
      "4                        India   6-8 year   \n",
      "\n",
      "                                              Skills  \\\n",
      "0  Aartificial intelligence,Data Analytics,Data s...   \n",
      "1  Aartificial intelligence,Algorithms,Data struc...   \n",
      "2  Aartificial intelligence,Analytical and Proble...   \n",
      "3  Aartificial intelligence,API,Data science tech...   \n",
      "4  Aartificial intelligence,API,Data science tech...   \n",
      "\n",
      "                                             Summary  \\\n",
      "0  About Quora:Quora’s mission is to grow and sha...   \n",
      "1  Minimum qualifications:Bachelor's degree in El...   \n",
      "2  About Quora:Quora’s mission is to grow and sha...   \n",
      "3  About Quora:Quora’s mission is to grow and sha...   \n",
      "4  About Quora:Quora’s mission is to grow and sha...   \n",
      "\n",
      "                     JobURL                   ScrapedAt  \n",
      "0  https://www.karkidi.com#  2025-05-24T11:39:50.086200  \n",
      "1  https://www.karkidi.com#  2025-05-24T11:39:50.086798  \n",
      "2  https://www.karkidi.com#  2025-05-24T11:39:50.086798  \n",
      "3  https://www.karkidi.com#  2025-05-24T11:39:50.086798  \n",
      "4  https://www.karkidi.com#  2025-05-24T11:39:50.086798  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def scrape_karkidi_jobs(keyword=\"data science\", pages=1, delay=1, save_csv=False, output_file=\"karkidi_jobs.csv\"):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    base_url = \"https://www.karkidi.com/Find-Jobs/{page}/all/India?search={query}\"\n",
    "    jobs_list = []\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        url = base_url.format(page=page, query=keyword.replace(' ', '%20'))\n",
    "        print(f\"Scraping page {page}: {url}\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve page {page}, status code: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        job_blocks = soup.find_all(\"div\", class_=\"ads-details\")\n",
    "\n",
    "        for job in job_blocks:\n",
    "            try:\n",
    "                title_tag = job.find(\"h4\")\n",
    "                title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
    "\n",
    "                company_tag = job.find(\"a\", href=lambda x: x and \"Employer-Profile\" in x)\n",
    "                company = company_tag.get_text(strip=True) if company_tag else \"\"\n",
    "\n",
    "                location_tag = job.find(\"p\")\n",
    "                location = location_tag.get_text(strip=True) if location_tag else \"\"\n",
    "\n",
    "                experience_tag = job.find(\"p\", class_=\"emp-exp\")\n",
    "                experience = experience_tag.get_text(strip=True) if experience_tag else \"\"\n",
    "\n",
    "                skills = \"\"\n",
    "                key_skills_tag = job.find(\"span\", string=\"Key Skills\")\n",
    "                if key_skills_tag:\n",
    "                    skills_p = key_skills_tag.find_next(\"p\")\n",
    "                    skills = skills_p.get_text(strip=True) if skills_p else \"\"\n",
    "\n",
    "                summary = \"\"\n",
    "                summary_tag = job.find(\"span\", string=\"Summary\")\n",
    "                if summary_tag:\n",
    "                    summary_p = summary_tag.find_next(\"p\")\n",
    "                    summary = summary_p.get_text(strip=True) if summary_p else \"\"\n",
    "\n",
    "                link_tag = job.find(\"a\", href=True)\n",
    "                job_url = \"https://www.karkidi.com\" + link_tag['href'] if link_tag else \"\"\n",
    "\n",
    "                jobs_list.append({\n",
    "                    \"Title\": title,\n",
    "                    \"Company\": company,\n",
    "                    \"Location\": location,\n",
    "                    \"Experience\": experience,\n",
    "                    \"Skills\": skills,\n",
    "                    \"Summary\": summary,\n",
    "                    \"JobURL\": job_url,\n",
    "                    \"ScrapedAt\": datetime.now().isoformat()\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing job block: {e}\")\n",
    "                continue\n",
    "\n",
    "        time.sleep(delay)  # Be nice to the server\n",
    "\n",
    "    df = pd.DataFrame(jobs_list)\n",
    "\n",
    "    if save_csv:\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved to {output_file}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example use:\n",
    "if __name__ == \"__main__\":\n",
    "    df_jobs = scrape_karkidi_jobs(keyword=\"data science\", pages=2, delay=1, save_csv=True)\n",
    "    print(df_jobs.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8dd9a9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
